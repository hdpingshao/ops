### 一、监控Linux服务器（使用node_exporter，其中node_exporter监控的是9100端口）

##### 1、在被监控主机上部署node_exporter采集器（用于采集服务器的基础监控数据，比如cpu、内存、磁盘、网络等）：

	# 下载node_exporter二进制包
	wget https://github.com/prometheus/node_exporter/releases/download/v0.17.0/node_exporter-0.17.0.linux-amd64.tar.gz
	# 解压二进制包
	tar zxvf node_exporter-0.17.0.linux-amd64.tar.gz 
	# 移动并重命名
	mv node_exporter-0.17.0.linux-amd64 /usr/local/node_exporter
	# 配置启动脚本
	cat << EOF > /usr/lib/systemd/system/node_exporter.service
	[Unit]
	Description=https://prometheus.io

	[Service]
	ExecStart=/usr/local/node_exporter/node_exporter --collector.systemd --collector.systemd.unit-whitelist=(docker|sshd|nginx).service
	Restart=on-failure

	[Install]
	WantedBy=multi-user.target
	EOF

	# 重载启动脚本
	systemctl daemon-reload
	# 启动node_exporter
	systemctl start node_exporter
	
##### 2、使用web查看监控数据的详细信息

    http://172.30.2.78:9100/metrics
	
##### 3、在prometheus上添加配置文件收集该主机上的node_exporter收集来的数据（例如我这边监控的主机IP：172.30.2.78）

	[root@localhost prometheus]# cat prometheus.yml
	# my global config
	global:
	  scrape_interval:     15s # Set the scrape interval to every 15 seconds. Default is every 1 minute.
	  evaluation_interval: 15s # Evaluate rules every 15 seconds. The default is every 1 minute.
	  # scrape_timeout is set to the global default (10s).

	# Alertmanager configuration
	alerting:
	  alertmanagers:
	  - static_configs:
		- targets:
		  # - alertmanager:9093

	# Load rules once and periodically evaluate them according to the global 'evaluation_interval'.
	rule_files:
	  # - "first_rules.yml"
	  # - "second_rules.yml"

	# A scrape configuration containing exactly one endpoint to scrape:
	# Here it's Prometheus itself.
	scrape_configs:
	  # The job name is added as a label `job=<job_name>` to any timeseries scraped from this config.
	  - job_name: 'prometheus'

		# metrics_path defaults to '/metrics'
		# scheme defaults to 'http'.

		static_configs:
		- targets: ['localhost:9090']

	  - job_name: 'node'
		static_configs:
		  - targets: ['172.30.2.78:9100']

##### 4、使用prometheus过滤查看来着主机172.30.2.78采集的所有数据信息（以node开头的基本就是通过node_exporter收集来的数据）

![image](https://github.com/hdpingshao/ops/tree/master/prometheus/images/prometheus5-1.jpg)

===

![image](https://github.com/hdpingshao/ops/tree/master/prometheus/images/prometheus5-2.jpg)

##### 5、监控由systemctl管理的服务（比如docker服务），其中active值为1表示docker服务正常启动

![image](https://github.com/hdpingshao/ops/tree/master/prometheus/images/prometheus5-3.jpg)

### 二、部署grafana(用于将收集上来的数据以图表等形式展示)，监控的是3000端口

	wget https://dl.grafana.com/oss/release/grafana-5.4.3-1.x86_64.rpm 
	sudo yum localinstall grafana-5.4.3-1.x86_64.rpm 
	
##### 示意图（初始登陆账号：admin，密码：admin）

![image](https://github.com/hdpingshao/ops/tree/master/prometheus/images/prometheus5-4.jpg)

##### 添加prometheus为grafana的数据源

![image](https://github.com/hdpingshao/ops/tree/master/prometheus/images/prometheus5-5.jpg)

===

![image](https://github.com/hdpingshao/ops/tree/master/prometheus/images/prometheus5-6.jpg)

##### 自定义数据模板或者导入网上现有的模板（我这边直接导入现有模板即可，自己自定义未必能设计的比人好，但是最终还是根据公司要求来定）

##### 推荐几个比较好的模板

- 监控主机： 9276
- 监控docker容器： 193
- 监控MySQL： 7362
- k8s-集群资源监控： 3119
- k8s-资源状态监控： 6417
- k8s-Node监控： 9276

##### 示意图

![image](https://github.com/hdpingshao/ops/tree/master/prometheus/images/prometheus5-7.jpg)

##### 查看最终数据展示图

![image](https://github.com/hdpingshao/ops/tree/master/prometheus/images/prometheus5-8.jpg)

### 三、监控docker容器（默认监听的是8080端口）

##### 1、被监控机上部署cAdvisor（Container Advisor）：用于收集正在运行的容器资源使用和性能信息。

	# 通过docker启动
	sudo docker run \
	  --volume=/:/rootfs:ro \
	  --volume=/var/run:/var/run:ro \
	  --volume=/sys:/sys:ro \
	  --volume=/var/lib/docker/:/var/lib/docker:ro \
	  --volume=/dev/disk/:/dev/disk:ro \
	  --publish=8080:8080 \
	  --detach=true \
	  --name=cadvisor \
	  google/cadvisor:latest

##### 使用web查看采集的数据
    http://172.30.2.78:8080/metrics
    
##### 在prometheus上添加监控该主机的配置

	[root@localhost prometheus]# cat prometheus.yml 
	# my global config
	global:
	  scrape_interval:     15s # Set the scrape interval to every 15 seconds. Default is every 1 minute.
	  evaluation_interval: 15s # Evaluate rules every 15 seconds. The default is every 1 minute.
	  # scrape_timeout is set to the global default (10s).

	# Alertmanager configuration
	alerting:
	  alertmanagers:
	  - static_configs:
		- targets:
		  # - alertmanager:9093

	# Load rules once and periodically evaluate them according to the global 'evaluation_interval'.
	rule_files:
	  # - "first_rules.yml"
	  # - "second_rules.yml"

	# A scrape configuration containing exactly one endpoint to scrape:
	# Here it's Prometheus itself.
	scrape_configs:
	  # The job name is added as a label `job=<job_name>` to any timeseries scraped from this config.
	  - job_name: 'prometheus'

		# metrics_path defaults to '/metrics'
		# scheme defaults to 'http'.

		static_configs:
		- targets: ['192.168.200.33:9090']

	  - job_name: 'docker'
		static_configs:
		- targets: ['172.30.2.78:8080']

	  - job_name: 'node'
		static_configs:
		  - targets: ['172.30.2.78:9100']

##### 在prometheus的web界面中可以查看以container开头的及为收集的容器的数据

![image](https://github.com/hdpingshao/ops/tree/master/prometheus/images/prometheus5-10.jpg)

##### 在grafana导入推荐的模板（193）展示容器监控图

![image](https://github.com/hdpingshao/ops/tree/master/prometheus/images/prometheus5-9.jpg)

### 监控MySQL数据库（默认监听的是9104端口）：用于收集MySQL性能信息

##### mysql_exporter部署（用于监控mysql数据库）

	# 下载mysql_exporter二进制包
	wget https://github.com/prometheus/mysqld_exporter/releases/download/v0.11.0/mysqld_exporter-0.11.0.linux-amd64.tar.gz
	# 解压二进制包
	tar zxvf mysqld_exporter-0.11.0.linux-amd64.tar.gz
	# 移动并重命名
	mv mysqld_exporter-0.11.0.linux-amd64 /usr/local/mysql_exporter
	# 配置启动脚本
	cat << EOF > /usr/lib/systemd/system/mysql_exporter.service
	[Unit]
	Description=https://prometheus.io

	[Service]
	ExecStart=/usr/local/mysql_exporter/mysql_exporter
	Restart=on-failure

	[Install]
	WantedBy=multi-user.target
	EOF

	# 重载启动脚本
	systemctl daemon-reload
	# 启动mysql_exporter
	systemctl start mysql_exporter
	
	# 创建账号并授权
	CREATE USER 'exporter'@'localhost' IDENTIFIED BY '123456' WITH MAX_USER_CONNECTIONS 3;
	GRANT PROCESS, REPLICATION CLIENT, SELECT ON *.* TO 'exporter'@'localhost';
	
##### 使用web查看采集的数据

    http://172.30.2.78:9104/metrics

##### 后续步骤大体同前面相似，这边就不做详解了